{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MostDeadDeveloper/ArabicCompetitiveProgramming/blob/master/challenge-5b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R3kW17I4hUN"
      },
      "source": [
        "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
        "\n",
        "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uQhBnor4hUQ"
      },
      "outputs": [],
      "source": [
        "NAME = \"Carl John Reyes Vinas\"\n",
        "COLLABORATORS = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzIHqS0W4hUS"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_R7Gp0e4hUS"
      },
      "source": [
        "# Challenge 5B: Solving $A\\vec{x} = \\lambda \\vec{x}$\n",
        "\n",
        "> _Linear relationships can be captured with a straight line on a graph. Linear relationships are easy to think about.... Linear systems have an important modular virtue: you can take them apart, and put them together again — the pieces add up._\n",
        ">\n",
        "> — James Gleick in _Chaos: Making a New Science_ (1987)\n",
        "\n",
        "The first part of this challenge should have given you a feel for how people turn the cranks and make matrices do useful stuff in practice (and by 'useful' I mean solving equations—you use equations in your daily life, right?)\n",
        "\n",
        "This part is about the theory.\n",
        "\n",
        "So much of what makes modern-day machine learning work won't make sense if you don't have a systematic understanding of what it means for something to change _linearly_: that is, if you double the input, you double the output. No hidden suprises.\n",
        "\n",
        "In some sense a linear transformation is the simplest nontrivial kind of response your system can have (as opposed to trivial ones like zero or constant responses), and it's no accident that we have collectively decided to make it the bedrock upon which we understand all other kinds of change.\n",
        "\n",
        "The behaviour of linear transformations only becomes clearer when we consider what they _preserve_ in the course of morphing spaces, and so by studying them in this challenge you should:\n",
        "\n",
        "* [ ] know what a vector space is, and what having such a structure implies about the stuff you can and cannot do\n",
        "* [ ] get into the habit of seeing beyond individual vectors and looking for their 'completions'; what this means will hopefully become clearer later on\n",
        "* [ ] be able to reformulate a thorny transformation into components that are easier to work with\n",
        "\n",
        "In particular, the main star of the show is the matrix equation $A\\vec{x} = \\lambda \\vec{x}$, and you may think of everything else that follows as mere stepping stones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBgD24vt4hUT"
      },
      "source": [
        "## Definition: (vector space)\n",
        "\n",
        "A **vector space** or **linear space** $V$ is a set of vectors where the following properties are always satisfied:\n",
        "\n",
        "1. [additive identity] The zero vector $\\vec{0}$ is in $V$; or in other words there's a vector $\\vec{0}$ in $V$ such that $\\vec{0} + \\vec{v} = \\vec{v}$ is always true.\n",
        "2. [additive inverse] Any vector $\\vec{v}$ in $V$ has a corresponding inverse vector $\\vec{-v}$ so that $\\vec{v} + (\\vec{-v}) = \\vec{0}$.\n",
        "3. [addition is commutative] $\\vec{v} + \\vec{w} = \\vec{v} + \\vec{w}$ for all $\\vec{v}, \\vec{w} \\in V$.\n",
        "4. [addition is associative] $\\vec{v_1} + (\\vec{v_2} + \\vec{v_3}) = (\\vec{v_1} + \\vec{v}) + \\vec{v}$ for all $\\vec{v_1}, \\vec{v_2}, \\vec{v_3} \\in V$.\n",
        "5. [multiplicative identity] For all $\\vec{v}$ in $V$, we have $1 \\vec{v} = \\vec{v}$.\n",
        "6. [multiplication is associative] $a(b\\vec{v}) = (ab)\\vec{v}$ for all scalars $a, b$ and $\\vec{v} \\in V$.\n",
        "7. [scalar addition is distributive] $(a + b)\\vec{v} = a\\vec{v} + b\\vec{v}$ for all scalars $a, b$ and $\\vec{v} \\in V$.\n",
        "8. [vector addition is distributive] $a(\\vec{v} + \\vec{w}) = a\\vec{v} + a\\vec{w}$ for all scalars $a$ and $\\vec{v}, \\vec{w} \\in V$.\n",
        "\n",
        "---\n",
        "\n",
        "Don't fret: by this point every single one of these properties should already be familiar to you: we're just consolidating them into one list.\n",
        "\n",
        "Also, this is what it really means for a _vector_ to be a _vector_. It's not about having magnitude or direction but about being an element of a _vector space_, obeying all the properties above and interacting with other vectors in precisely the ways we have listed.\n",
        "\n",
        "This also means that there are 'vectors' that are not necessarily floating arrows in space, as the following example will show.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngbThjBA4hUU"
      },
      "source": [
        "## Example: Light switches\n",
        "\n",
        "Consider $n$ light switches, each of which can take either two states: ON or OFF. Form the set $V$ of possible _configurations_ these light switches can be in, so for example:\n",
        "\n",
        "$$\n",
        "\\vec{v} = \\begin{bmatrix} ON \\\\ ON \\\\ \\dots \\\\ ON \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "is one such configuration.\n",
        "\n",
        "Now, let's say 'adding' two configurations means, for each corresponding pair of switch entries, you leave it ON if there is _exactly_ one switch entry in the pair that's ON. Example:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "ON \\\\\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Furthermore, 'multiplying' a configuration by ON means _leaving ON_ only the switches that were already ON, and multiplying by OFF means turning OFF everything. So:\n",
        "\n",
        "$$\n",
        "(ON)\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\n",
        "(OFF)\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "OFF \\\\\n",
        "OFF \\\\\n",
        "OFF \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Then $V$ is a vector space.\n",
        "\n",
        "---\n",
        "\n",
        "You might also remember the notion of a **vector subspace** from the first session. We can now restate its definition more compactly: it is a subset of a vector space $V$ which is also a vector space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT6s9gAm4hUU"
      },
      "source": [
        "## Problem 1: Show that $V$ in the light switch example is a vector space.\n",
        "\n",
        "Note that your only scalars are in the set $\\{ON, OFF\\}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer:\n",
        "\n",
        "A good way to answer this is proving that all possible values and combinations in this vector space applies to the needed properties of any vector space.\n",
        "\n",
        "Luckily, we can represent all possible combinations of the two possible values in this vector space through these two vectors:\n",
        "- $ \\vec{v_1}  = \\begin{bmatrix}\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "\\vec{v_2}  =\n",
        "\\begin{bmatrix}\n",
        "ON \\\\\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "We can also represent the only two possible values of the vector space which is $a=ON$ and $b=OFF$.\n",
        "\n",
        "Throughout the different properties, we will use $ \\vec{v_1}$, $ \\vec{v_2}$, $a$ and $b$ to prove that these properties work for all values and combinations of the vector space.\n",
        "\n",
        "We consider V as a vector space since it follows all the given properties below:\n",
        "1. **[additive identity]**: this property is fullfiled since there exists a vector $\\vec{0}$ in $V$ such that $\\vec{0} + \\vec{v} = \\vec{0}$ is always true. Specifically this is a vector where it only contains b.\n",
        "- $\\vec{0} = \\begin{bmatrix} OFF \\\\ OFF \\\\ \\dots \\\\ OFF \\end{bmatrix} or \\begin{bmatrix} OFF \\\\ \\end{bmatrix} $\n",
        "\n",
        "2. **[additive inverse]**: This property applies, since every $\\vec{v}$ in $V$ has a corresponding inverse vector $\\vec{-v}$ so that $\\vec{v} + (\\vec{-v}) = \\vec{0}$. The corresponding inverse vector is itself. This can be further seen below on the given vectors:\n",
        "- $ \\vec{v_1} + \\vec{v_1} =\\vec{0} $\n",
        "- $ \\vec{v_2} + \\vec{v_2} = \\vec{0} $\n",
        "3. **[addition is commutative]**: This property applies as well for all possible combinations and values in the vector space as we can see below:\n",
        "- $ \\vec{v_1} + \\vec{v_2} = \\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}$\n",
        "- $ \\vec{v_2} + \\vec{v_1} = \\begin{bmatrix}\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix} $\n",
        "4. **[addition is associative]**: this property also applies for this vector space, we can prove this by trying out if the property holds for two possible cases, which is:\n",
        "- ($ \\vec{v_1} + \\vec{v_2}) + \\vec{v_2} = $\n",
        "- ($ \\vec{v_1} + \\vec{v_2}) + \\vec{v_1}$\n",
        "\n",
        "For the first Case:\n",
        "- ($ \\vec{v_1} + \\vec{v_2}) + \\vec{v_1} = \\begin{bmatrix}\n",
        "ON \\\\\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$\n",
        "- $ \\vec{v_1} + (\\vec{v_2} + \\vec{v_1}) = \\begin{bmatrix}\n",
        "ON \\\\\n",
        "OFF \\\\\n",
        "ON \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "And thus since they are both equal, the property holds for the first case.\n",
        "\n",
        "For the Second Case:\n",
        "- ($ \\vec{v_1} + \\vec{v_2}) + \\vec{v_2} = \\begin{bmatrix}\n",
        "OFF \\\\\n",
        "OFF \\\\\n",
        "OFF \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "- $ \\vec{v_1} + (\\vec{v_2} + \\vec{v_2}) = \\begin{bmatrix}\n",
        "OFF \\\\\n",
        "OFF \\\\\n",
        "OFF \\\\\n",
        "OFF\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n",
        "Since for both case holds, the \"addition is associative\" holds for all values and combinations.\n",
        "\n",
        "5. **[multiplicative identity]**: For all $\\vec{v}$, we have $(ON)(\\vec{v}) = \\vec{v}$\n",
        "\n",
        "6. **[multiplication is associative]** $a(b\\vec{v}) = (ab)\\vec{v}$ for all scalars $a, b$ and $\\vec{v} \\in V$.\n",
        "\n",
        "Supposing $\\vec{v}$ is any vector in this vector space,\n",
        "\n",
        "If  $a = OFF$, the property still holds:\n",
        "- $a(b\\vec{v}) = (ab)\\vec{v}$\n",
        "- $OFF(b\\vec{v}) = ((OFF)(b))\\vec{v}$\n",
        "- $OFF(b\\vec{v}) = (OFF)\\vec{v}$\n",
        "- $OFF = OFF$\n",
        " - anything multiplied to OFF, results in the zero vector ot a vector with OFF.\n",
        "\n",
        "\n",
        "If  $a = ON$ , the property still holds:\n",
        "- $a(b\\vec{v}) = (ab)\\vec{v}$\n",
        "- $ON(b\\vec{v}) = ((ON)(b))\\vec{v}$\n",
        "- $(b\\vec{v}) = (b)\\vec{v}$\n",
        "- $(b)\\vec{v} = (b)\\vec{v}$\n",
        " - due to the multiplicative identity.\n",
        "\n",
        "Since all values are equal for all cases, this property works for this vector space.\n",
        "\n",
        "7. **[scalar addition is distributive]** $(a + b)\\vec{v} = a\\vec{v} + b\\vec{v}$ for all scalars $a, b$ and $\\vec{v} \\in V$.\n",
        "\n",
        "If all $a$ $b$, $\\vec{v}$ is OFF case:\n",
        "- If this is the case, the property still holds:\n",
        "- $(a + b)\\vec{v} = a\\vec{v} + b\\vec{v}$\n",
        "- $(a + b)\\vec{v} = (OFF)\\vec{v} + (OFF)\\vec{v}$\n",
        "- $(OFF + OFF)\\vec{v} = (OFF)\\vec{v} + (OFF)\\vec{v}$\n",
        "- $(OFF)\\vec{v} = (OFF)\\vec{v} + (OFF)\\vec{v}$\n",
        " - due to the additive identity\n",
        "- $(OFF)\\vec{v} = (OFF)\\vec{v} + (OFF)\\vec{v}$\n",
        "- $OFF = OFF + OFF$\n",
        "- $OFF = OFF$\n",
        "\n",
        "If a and b is ON case:\n",
        "- $(a + b)\\vec{v} = a\\vec{v} + b\\vec{v}$\n",
        "- $(ON + ON)\\vec{v} = ON(\\vec{v}) + ON(\\vec{v})$\n",
        "- $(OFF)\\vec{v} = ON(\\vec{v}) + ON(\\vec{v})$\n",
        "- $(OFF)\\vec{v} = \\vec{v} + \\vec{v}$\n",
        " - due to the multiplicative identity.\n",
        "- $OFF = \\vec{v} + \\vec{v}$\n",
        "- $OFF = OFF$\n",
        " - following the additive inverse property\n",
        "\n",
        "For a=ON b=OFF or a=OFF, b=ON case:\n",
        "- $(a + b)\\vec{v} = a\\vec{v} + b\\vec{v}$\n",
        "- $(ON + OFF)\\vec{v} = ON\\vec{v} + OFF\\vec{v}$\n",
        "- $(ON)\\vec{v} = ON\\vec{v} + OFF\\vec{v}$\n",
        "- $\\vec{v} = \\vec{v} + OFF $\n",
        "- $\\vec{v} = \\vec{v}$\n",
        " - following the additive identity property.\n",
        "\n",
        "8. **[vector addition is distributive]** $a(\\vec{v_1} + \\vec{v_2}) = a\\vec{v_1} + a\\vec{v_2}$ for all scalars $a$ and $\\vec{v_1}, \\vec{v_2} \\in V$.\n",
        "\n",
        "Applying this property, to both $\\vec{v_1} + \\vec{v_2}$:\n",
        "\n",
        "Supposing $a$ is a value of OFF:\n",
        "- $a(\\vec{v_1} + \\vec{v_2}) = a\\vec{v_1} + a\\vec{v_2}$\n",
        "- $OFF(\\vec{v_1} + \\vec{v_2}) = a\\vec{v_1} + a\\vec{v_2}$\n",
        "- $OFF(\\vec{v_1} + \\vec{v_2}) = OFF\\vec{v_1} + OFF\\vec{v_2}$\n",
        "- $OFF(\\begin{bmatrix}OFF \\\\ON \\\\ON \\\\OFF\\end{bmatrix}) = (OFF)\\vec{v_1} + (OFF)\\vec{v_2}$\n",
        "- $OFF(\\begin{bmatrix}OFF \\\\ON \\\\ON \\\\OFF\\end{bmatrix}) = (OFF) + (OFF)$\n",
        "- $OFF(\\begin{bmatrix}OFF \\\\ON \\\\ON \\\\OFF\\end{bmatrix}) = (OFF) + (OFF)$\n",
        "- $OFF = (OFF) + (OFF)$\n",
        "- $OFF = OFF$\n",
        "\n",
        "Supposing $a$ is a value of ON:\n",
        "- $a(\\vec{v_1} + \\vec{v_2}) = a\\vec{v_1} + a\\vec{v_2}$\n",
        "- $ON(\\vec{v_1} + \\vec{v_2}) = a\\vec{v_1} + a\\vec{v_2}$\n",
        "- $ON(\\vec{v_1} + \\vec{v_2}) = ON\\vec{v_1} + ON\\vec{v_2}$\n",
        "- $ON(\\begin{bmatrix}OFF \\\\ON \\\\ON \\\\OFF\\end{bmatrix}) = (ON)\\vec{v_1} + (ON)\\vec{v_2}$\n",
        "- $ON(\\begin{bmatrix}OFF \\\\ON \\\\ON \\\\OFF\\end{bmatrix}) = \\vec{v_1} + \\vec{v_2}$\n",
        "- $\\begin{bmatrix}OFF \\\\ON \\\\ON \\\\OFF\\end{bmatrix} = \\vec{v_1} + \\vec{v_2}$\n",
        "- $\\begin{bmatrix}OFF \\\\ON \\\\ON \\\\OFF\\end{bmatrix} = \\begin{bmatrix}OFF \\\\ON \\\\ON \\\\OFF\\end{bmatrix}$\n",
        "\n"
      ],
      "metadata": {
        "id": "3fa_s_AibED1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVroOKMQ4hUU"
      },
      "source": [
        "## Definition: (linear combination)\n",
        "\n",
        "A **linear combination** of vectors $\\vec{v_1}, \\dots, \\vec{v_n}$ in a vector space $V$ is simply a vector $\\vec{v}$ in $V$ where:\n",
        "\n",
        "$$\n",
        "\\vec{v} = \\sum_{i=1}^n {a_i v_i}\n",
        "$$\n",
        "\n",
        "for scalars $a_i$. In other words, it is a _weighted sum_ of $\\vec{v_1}, \\dots, \\vec{v_n}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7nIs_aI4hUV"
      },
      "source": [
        "## Definition: (span)\n",
        "\n",
        "A set of vectors $\\vec{v_1}, \\dots, \\vec{v_n}$ are said to **span** $V$ if and only if every vector in $V$ can be written as a linear combination of $\\vec{v_1}, \\dots, \\vec{v_n}$.\n",
        "\n",
        "Alternatively, the **span** of $\\vec{v_1}, \\dots, \\vec{v_n}$ is the set $\\text{Span } (\\vec{v_1}, \\dots, \\vec{v_n})$ of all their linear combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk8_lQtE4hUV"
      },
      "source": [
        "## Example: Standard basis vectors\n",
        "\n",
        "The standard basis vectors $\\vec{e_1} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$ and $\\vec{e_2} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$ span $\\mathbb{R}^2$. Why? Because you can write any vector $\\vec{v}$ in $\\mathbb{R}^2$ as:\n",
        "\n",
        "$$\n",
        "\\vec{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = v_1 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + v_2 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viI31wwu4hUV"
      },
      "source": [
        "## Definition: (linear independence)\n",
        "\n",
        "A set of vectors $\\vec{v_1}, \\dots, \\vec{v_n}$ are said to be **linearly independent** if none of $\\vec{v_i}$ can be written as a linear combination of the others.\n",
        "\n",
        "Equivalently, they are linearly independent if the only solution to\n",
        "\n",
        "$$\n",
        "a_1 \\vec{v_1} + \\dots + a_n \\vec{v_n} = \\vec{0}\n",
        "$$\n",
        "\n",
        "is for all the $a_i$ to be equal to zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST8ohPMz4hUV"
      },
      "source": [
        "## Example: Standard basis vectors are linearly independent\n",
        "\n",
        "The standard basis vectors in $\\mathbb{R}^3$: $\\vec{e_1} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $\\vec{e_2} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, and $\\vec{e_3} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$\n",
        "\n",
        "are linearly independent. To see this, form:\n",
        "\n",
        "$$\n",
        "a_1 \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
        "+\n",
        "a_2 \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}\n",
        "+\n",
        "a_3 \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "from which it is clear that $a_1 = a_2 = a_3 = 0$.\n",
        "\n",
        "---\n",
        "\n",
        "To make our lives easier, we shall denote the set of vectors $\\vec{v_1}, \\dots, \\vec{v_n}$ as $\\{ \\vec{v_i} \\}_{i=1}^n$ or simply $\\{\\vec{v_i}\\}$ if the number of elements is clear from context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYglgazk4hUV"
      },
      "source": [
        "## Proposition I: Linear independence and span\n",
        "\n",
        "Let $\\{\\vec{v_i}\\}_{i=1}^k$ be vectors in $\\mathbb{R}^n$. Form the $n \\times k$ matrix $A = [\\vec{v_1}, \\dots, \\vec{v_k}]$ whose ith column is simply $\\vec{v_i}$. Then:\n",
        "\n",
        "1. $\\{\\vec{v_i}\\}$ are linearly independent if the row-reduced matrix $\\tilde{A}$ has a pivotal 1 in every column.\n",
        "2. $\\{\\vec{v_i}\\}$ span $\\mathbb{R}^n$ if and only if $\\tilde{A}$ has a pivotal 1 in every row."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVwlgvlt4hUV"
      },
      "source": [
        "## Problem 2: Using Proposition I\n",
        "\n",
        "a. Given $\\vec{w_1} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $\\vec{w_2} = \\begin{bmatrix} 1 \\\\ -1 \\\\ 1 \\end{bmatrix}$, $\\vec{w_3} = \\begin{bmatrix} 3 \\\\ 0 \\\\ 2 \\end{bmatrix}$, and $\\vec{v} = \\begin{bmatrix} 3 \\\\ 3 \\\\ 1 \\end{bmatrix}$\n",
        "\n",
        "Is $\\vec{v}$ in the span of $\\{w_i\\}$? Check this using row reduction.\n",
        "\n",
        "b. Given $\\vec{w_1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}$, $\\vec{w_2} = \\begin{bmatrix} -2 \\\\ 1 \\\\ 2 \\end{bmatrix}$, $\\vec{w_3} = \\begin{bmatrix} -1 \\\\ 1 \\\\ -1 \\end{bmatrix}$\n",
        "\n",
        "Are they linearly independent? Do they span $\\mathbb{R}^3$? Check these using row reduction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-84b14f0d1ba32876",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "seXD3xrF4hUW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8af1910d-3560-44d8-fb99-0bfc2d2b7d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer to Problem 2.a\n",
            "Row Operations for First Initial Matrix\n",
            "Initial Matrix #1:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 2.,  1.,  3.,  3.],\n",
              "        [ 1., -1.,  0.,  3.],\n",
              "        [ 1.,  1.,  2.,  1.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #1 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1. ,  0.5,  1.5,  1.5],\n",
              "        [ 1. , -1. ,  0. ,  3. ],\n",
              "        [ 1. ,  1. ,  2. ,  1. ]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #1 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1. ,  0.5,  1.5,  1.5],\n",
              "        [ 0. , -1.5, -1.5,  1.5],\n",
              "        [ 1. ,  1. ,  2. ,  1. ]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #1 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1. ,  0.5,  1.5,  1.5],\n",
              "        [ 0. , -1.5, -1.5,  1.5],\n",
              "        [ 0. ,  0.5,  0.5, -0.5]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #1 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1. ,  0.5,  1.5,  1.5],\n",
              "        [ 0. ,  1. ,  1. , -1. ],\n",
              "        [ 0. ,  0.5,  0.5, -0.5]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #1 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1. ,  0. ,  1. ,  2. ],\n",
              "        [ 0. ,  1. ,  1. , -1. ],\n",
              "        [ 0. ,  0.5,  0.5, -0.5]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #1 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1.,  0.,  1.,  2.],\n",
              "        [ 0.,  1.,  1., -1.],\n",
              "        [ 0.,  0.,  0.,  0.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reached Row Reduced Form :\n",
            "\n",
            "Using the given values derived from the matrix form in reduced row echelor form above,\n",
            "we get a=2, b=-1, c=0.\n",
            "\n",
            "\n",
            "Since there exists a linear combination of all three vectors in w for vector v,\n",
            "Therefore, vector v must be part of their span.\n",
            "\n",
            "\n",
            "We can verify this by computing a*v_1 + b*v_2 + c*v_3\n",
            "\n",
            "Result is :[[3.]\n",
            " [3.]\n",
            " [1.]]\n",
            "Answer to Problem 2.b\n",
            "Row Operations for Second Initial Matrix\n",
            "Initial Matrix #2:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1, -2, -1,  0],\n",
              "        [ 2,  1,  1,  0],\n",
              "        [ 3,  2, -1,  0]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #2 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1., -2., -1.,  0.],\n",
              "        [ 0.,  5.,  3.,  0.],\n",
              "        [ 3.,  2., -1.,  0.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #2 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1., -2., -1.,  0.],\n",
              "        [ 0.,  5.,  3.,  0.],\n",
              "        [ 0.,  8.,  2.,  0.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #2 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1. , -2. , -1. ,  0. ],\n",
              "        [ 0. ,  1. ,  0.6,  0. ],\n",
              "        [ 0. ,  8. ,  2. ,  0. ]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #2 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[1. , 0. , 0.2, 0. ],\n",
              "        [0. , 1. , 0.6, 0. ],\n",
              "        [0. , 8. , 2. , 0. ]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #2 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1. ,  0. ,  0.2,  0. ],\n",
              "        [ 0. ,  1. ,  0.6,  0. ],\n",
              "        [ 0. ,  0. , -2.8,  0. ]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #2 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[1. , 0. , 0.2, 0. ],\n",
              "        [0. , 1. , 0.6, 0. ],\n",
              "        [0. , 0. , 1. , 0. ]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #2 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[1.00000000e+00, 0.00000000e+00, 1.22124533e-16, 0.00000000e+00],\n",
              "        [0.00000000e+00, 1.00000000e+00, 6.00000000e-01, 0.00000000e+00],\n",
              "        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #2 after Operation :\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1.00000000e+00,  0.00000000e+00,  1.22124533e-16,\n",
              "          0.00000000e+00],\n",
              "        [ 0.00000000e+00,  1.00000000e+00, -2.22044605e-17,\n",
              "          0.00000000e+00],\n",
              "        [ 0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
              "          0.00000000e+00]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: C^13 and C^23 is supposed to be zero, but I think numpy is trying to get a smaller approximation to zero instead.\n",
            "Since the matrix can be row reduced and results in no solutions, this means all three vectors can only be linearly independent.\n",
            "And since all three of them are linearly independent, this means they span all of the 3D space (R^3)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "# Problem 2a\n",
        "# To check if vector v is in the span of w, we can use row reduction and find if there exists scalars for\n",
        "# w_1, w_2, and w_3 where it would result in vector v.\n",
        "# Listed below are the step by step row reduction for these vectors:\n",
        "print('Answer to Problem 2.a')\n",
        "print('Row Operations for First Initial Matrix')\n",
        "initial_matrix_1 = np.matrix(\n",
        "    [[2., 1., 3,3],\n",
        "    [1., -1., 0,3],\n",
        "    [1., 1., 2,1]]\n",
        ")\n",
        "print('Initial Matrix #1:')\n",
        "display(initial_matrix_1)\n",
        "\n",
        "operation = np.matrix(\n",
        "    [[1/2, 0, 0.],\n",
        "    [0, 1, 0.],\n",
        "    [0, 0, 1.]]\n",
        ")\n",
        "initial_matrix_1 = operation * initial_matrix_1\n",
        "\n",
        "print('Matrix #1 after Operation :')\n",
        "display(initial_matrix_1)\n",
        "\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0.],\n",
        "    [-1, 1, 0.],\n",
        "    [0, 0, 1.]]\n",
        ")\n",
        "initial_matrix_1 = operation * initial_matrix_1\n",
        "\n",
        "print('Matrix #1 after Operation :')\n",
        "display(initial_matrix_1)\n",
        "\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0.],\n",
        "    [0, 1, 0.],\n",
        "    [-1, 0, 1.]]\n",
        ")\n",
        "initial_matrix_1 = operation * initial_matrix_1\n",
        "\n",
        "print('Matrix #1 after Operation :')\n",
        "display(initial_matrix_1)\n",
        "\n",
        "# ===============\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0.],\n",
        "    [0, -2/3, 0.],\n",
        "    [0, 0, 1.]]\n",
        ")\n",
        "initial_matrix_1 = operation * initial_matrix_1\n",
        "\n",
        "print('Matrix #1 after Operation :')\n",
        "display(initial_matrix_1)\n",
        "\n",
        "# ===============\n",
        "operation = np.matrix(\n",
        "    [[1, -1/2, 0.],\n",
        "    [0, 1, 0.],\n",
        "    [0, 0, 1.]]\n",
        ")\n",
        "initial_matrix_1 = operation * initial_matrix_1\n",
        "\n",
        "print('Matrix #1 after Operation :')\n",
        "display(initial_matrix_1)\n",
        "\n",
        "# ===============\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0.],\n",
        "    [0, 1, 0.],\n",
        "    [0, -1/2, 1.]]\n",
        ")\n",
        "initial_matrix_1 = operation * initial_matrix_1\n",
        "\n",
        "print('Matrix #1 after Operation :')\n",
        "display(initial_matrix_1)\n",
        "print('Reached Row Reduced Form :')\n",
        "print(\"\"\"\n",
        "Using the given values derived from the matrix form in reduced row echelor form above,\n",
        "we get a=2, b=-1, c=0.\n",
        "\"\"\")\n",
        "print(\"\"\"\n",
        "Since there exists a linear combination of all three vectors in w for vector v,\n",
        "Therefore, vector v must be part of their span.\n",
        "\"\"\")\n",
        "\n",
        "initial_matrix_1_orig = np.matrix(\n",
        "    [[2., 1., 3,3],\n",
        "    [1., -1., 0,3],\n",
        "    [1., 1., 2,1]]\n",
        ")\n",
        "a = 2\n",
        "b = -1\n",
        "c = 0\n",
        "\n",
        "\n",
        "print(\"\"\"\n",
        "We can verify this by computing a*v_1 + b*v_2 + c*v_3\n",
        "\"\"\")\n",
        "print('Result is :{}'.format(\n",
        "    initial_matrix_1_orig[:,0] * a + initial_matrix_1_orig[:,1] * b + initial_matrix_1_orig[:,2] * c\n",
        "))\n",
        "\n",
        "print('Answer to Problem 2.b')\n",
        "print('Row Operations for Second Initial Matrix')\n",
        "initial_matrix_2 = np.matrix(\n",
        "    [[1, -2, -1,0],\n",
        "    [2, 1, 1,0],\n",
        "    [3, 2, -1,0]]\n",
        ")\n",
        "print('Initial Matrix #2:')\n",
        "display(initial_matrix_2)\n",
        "\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0.],\n",
        "    [-2, 1, 0.],\n",
        "    [0, 0, 1.]]\n",
        ")\n",
        "initial_matrix_2 = operation * initial_matrix_2\n",
        "\n",
        "print('Matrix #2 after Operation :')\n",
        "display(initial_matrix_2)\n",
        "\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0.],\n",
        "    [0, 1, 0.],\n",
        "    [-3, 0, 1.]]\n",
        ")\n",
        "initial_matrix_2 = operation * initial_matrix_2\n",
        "\n",
        "print('Matrix #2 after Operation :')\n",
        "display(initial_matrix_2)\n",
        "\n",
        "####\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0.],\n",
        "    [0, 1/5, 0.],\n",
        "    [0, 0, 1.]]\n",
        ")\n",
        "initial_matrix_2 = operation * initial_matrix_2\n",
        "\n",
        "print('Matrix #2 after Operation :')\n",
        "display(initial_matrix_2)\n",
        "\n",
        "####\n",
        "operation = np.matrix(\n",
        "    [[1, 2, 0.],\n",
        "    [0, 1, 0.],\n",
        "    [0, 0, 1.]]\n",
        ")\n",
        "initial_matrix_2 = operation * initial_matrix_2\n",
        "\n",
        "print('Matrix #2 after Operation :')\n",
        "display(initial_matrix_2)\n",
        "\n",
        "####\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0.],\n",
        "    [0, 1, 0.],\n",
        "    [0, -8, 1.]]\n",
        ")\n",
        "initial_matrix_2 = operation * initial_matrix_2\n",
        "\n",
        "print('Matrix #2 after Operation :')\n",
        "display(initial_matrix_2)\n",
        "\n",
        "####\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0.],\n",
        "    [0, 1, 0.],\n",
        "    [0, 0, -5/14.]]\n",
        ")\n",
        "initial_matrix_2 = operation * initial_matrix_2\n",
        "\n",
        "print('Matrix #2 after Operation :')\n",
        "display(initial_matrix_2)\n",
        "\n",
        "####\n",
        "operation = np.matrix(\n",
        "    [[1, 0, -.2],\n",
        "    [0, 1, 0.],\n",
        "    [0, 0, 1.]]\n",
        ")\n",
        "initial_matrix_2 = operation * initial_matrix_2\n",
        "\n",
        "print('Matrix #2 after Operation :')\n",
        "display(initial_matrix_2)\n",
        "\n",
        "####\n",
        "operation = np.matrix(\n",
        "    [[1, 0, 0],\n",
        "    [0, 1, -3/5],\n",
        "    [0, 0, 1.]]\n",
        ")\n",
        "initial_matrix_2 = operation * initial_matrix_2\n",
        "\n",
        "print('Matrix #2 after Operation :')\n",
        "display(initial_matrix_2)\n",
        "print('Note: C^13 and C^23 is supposed to be zero, but I think numpy is trying to get a smaller approximation to zero instead.')\n",
        "print('Since the matrix can be row reduced and results in no solutions, this means all three vectors can only be linearly independent.')\n",
        "print('And since all three of them are linearly independent, this means they span all of the 3D space (R^3)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA7USvDO4hUW"
      },
      "source": [
        "## Problem (bonus): Prove Proposition I\n",
        "\n",
        "<details>\n",
        "<summary>Hint</summary>\n",
        "The theorem you need is in Challenge 5A.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-e25962cf8121213d",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "D5GLev2C4hUW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7Dc-NUE4hUW"
      },
      "source": [
        "## Definition: (basis)\n",
        "\n",
        "A set $\\{\\vec{v_i}\\}$ of vectors in $V$ is called a **basis** of $V$ if _any_ of the following conditions hold:\n",
        "\n",
        "1. The set is a maximally linearly independent set: it is linearly independent, and if you add one more vector, it will no longer be.\n",
        "2. The set is a minimally spanning set: it spans $V$, and if you drop one vector, it will no longer span $V$.\n",
        "3. The set is a linearly independent set spanning $V$.\n",
        "\n",
        "---\n",
        "\n",
        "Choosing a basis for a vector space is in some sense, choosing its _axes_. Each basis vector represents an axis and its magnitude determines the axis' unit of length.\n",
        "\n",
        "Also, it can be shown that all bases for a vector space $V$ will have the same number of elements, which we call the **dimension** of $V$ or $\\text{dim }V$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oga6poR4hUW"
      },
      "source": [
        "## Example: Standard basis\n",
        "\n",
        "We now understand why $\\{\\vec{e_i}\\}_{i=1}^n$ are called _basis_ vectors: because they form a basis of $\\mathbb{R}^n$.\n",
        "\n",
        "To see this, verify that they span $\\mathbb{R}^n$ and are linearly independent. So by Proposition I, they must serve as a basis (aka the **standard basis**) for $\\mathbb{R}^n$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4wcXNFq4hUW"
      },
      "source": [
        "**Q**: Is $\\vec{e} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$ part of $\\mathbb{R}^2$?\n",
        "\n",
        "<details>\n",
        "<summary>Answer</summary>\n",
        "No, because it has three entries.\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "Point is, there's a different standard basis for every $k$ in $\\mathbb{R}^k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW4EW2904hUW"
      },
      "source": [
        "## Problem 3: Find a basis for $\\mathbb{R}^2$ that isn't the standard basis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Either (1,1) or (-1,1), or (2,3) works\n",
        "\n",
        "Show solution here through checking any of those vectors if its linearly independent and spans all of V"
      ],
      "metadata": {
        "id": "zzikn9q5TvkI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFgzoup54hUW"
      },
      "source": [
        "## Definition: (orthonormal basis)\n",
        "\n",
        "A set of vectors $\\{\\vec{v_i}\\}$ is **orthonormal** if each vector is orthogonal to every other one in the set, and if all of them are of length 1. That is:\n",
        "\n",
        "$$\n",
        "\\vec{v_i} \\cdot \\vec{v_j} = 0\n",
        "$$\n",
        "\n",
        "when $i \\neq j$, and $|\\vec{v_k}| = 1$ for all $\\vec{v_k}$.\n",
        "\n",
        "These vectors then are said to form an **orthonormal basis** for $\\text{Span } \\{\\vec{v_i}\\}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJQhQK7e4hUX"
      },
      "source": [
        "## Example: The standard basis is orthonormal\n",
        "\n",
        "We know that $\\{\\vec{e_i}_{i=1}^n\\}$ is a basis for $\\mathbb{R}^n$, and they are clearly of length 1. To show then that it is an orthonormal basis for $\\mathbb{R}^n$, it is enough to verify that they are all orthogonal to each other.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbMXOOMI4hUX"
      },
      "source": [
        "## Definition: (orthogonal matrix)\n",
        "\n",
        "An $n \\times n$ matrix $A$ is **orthogonal** if it satisfies any of the following conditions:\n",
        "\n",
        "1. $A A^T = A^T A = I$, or in other words its transpose is its inverse: $A^T = A^{-1}$.\n",
        "2. The columns of $A$ form an orthonormal basis of $\\mathbb{R}^n$.\n",
        "3. For any $\\vec{v}, \\vec{w} \\in \\mathbb{R}^n$,\n",
        "\n",
        "$$\n",
        "A\\vec{v} \\cdot A\\vec{w} = \\vec{v} \\cdot \\vec{w}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esyLhevi4hUX"
      },
      "source": [
        "## Definition: (kernel and image)\n",
        "\n",
        "Let $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear transformation.\n",
        "\n",
        "1. The **kernel** of $T$ or $\\text{ker } T$ is the set of vectors $\\vec{x} \\in \\mathbb{R}^n$ such that $T(\\vec{x}) = \\vec{0}$.\n",
        "2. The **image** of $T$ or $\\text{img } T$ is the set of vectors $\\vec{w} \\in \\mathbb{R}^m$ such that there exists a vector $\\vec{v} \\in \\mathbb{R}^n$ with $T(\\vec{v}) = \\vec{w}$\n",
        "\n",
        "---\n",
        "\n",
        "In other words, the _kernel_ tells you all the vectors a linear transformation sends to $\\vec{0}$, while the _image_ is the set of vectors the transformation can possibly output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cArsz8qB4hUY"
      },
      "source": [
        "## Example: Vector in a kernel\n",
        "\n",
        "Let $A = \\begin{bmatrix} 1 & 1 & 1 \\\\ 2 & -1 & 1 \\end{bmatrix}$ be (the matrix that represents) a linear transformation. Then $\\vec{v} = \\begin{bmatrix} -2 \\\\ -1 \\\\ 3 \\end{bmatrix}$ is in $\\text{ker } A$, because:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 1 & 1 & 1 \\\\ 2 & -1 & 1 \\end{bmatrix}\n",
        "\\begin{bmatrix} -2 \\\\ -1 \\\\ 3 \\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "If $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ is a linear transformation, then $\\text{ker }T$ is a vector subspace of $\\mathbb{R}^n$ and $\\text{img }T$ is a vector subspace of $\\mathbb{R}^m$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcBg54hp4hUY"
      },
      "source": [
        "## Proposition II: Kernel, image, and solutions to systems of equations\n",
        "\n",
        "Let $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear transformation. Then the system of (linear) equations $T(\\vec{x}) = \\vec{b}$ has:\n",
        "\n",
        "1. at _most_ one solution for every $\\vec{b} \\in \\mathbb{R}^n$ if and only if $\\text{ker }T = \\{\\vec{0}\\}$\n",
        "\n",
        "2. at _least_ one solution for every $\\vec{b} \\in \\mathbb{R}^n$ if and only if $\\text{img }T = \\mathbb{R}^m$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbPuy6io4hUY"
      },
      "source": [
        "## Problem (bonus): Prove Proposition II."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-02924f67aa5b16c5",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "uq7m8Qlz4hUY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJaLKGLd4hUZ"
      },
      "source": [
        "## Proposition III: Dimension and rank\n",
        "\n",
        "Let $T: \\mathbb{R}^n \\to \\mathbb{R}^m$ be a linear transformation. Then:\n",
        "\n",
        "$$\n",
        "\\text{dim }(\\text{ker }T) + \\text{dim }(\\text{img }T) = n\n",
        "$$\n",
        "\n",
        "(the dimension of $\\mathbb{R}^n$)\n",
        "\n",
        "Furthermore, $\\text{dim }(\\text{img }T)$ is called the **rank** of T.\n",
        "\n",
        "---\n",
        "\n",
        "It can be shown by a somewhat involved proof that, if $\\tilde{T}$ is the row-reduced form of the matrix $[T]$ representing $T$, then $\\text{dim }(\\text{img }T)$ is equal to the number of pivotal columns, while $\\text{dim }(\\text{ker }T)$ is equal to the non-pivotal columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h6asw5B4hUZ"
      },
      "source": [
        "## Example: Fibonacci sequence\n",
        "\n",
        "The Fibonacci numbers $1, 1, 2, 3, 5, 8, 13, \\dots$ are usually defined by the following procedure: take the last two numbers and then add them together to get the next one. In equation form:\n",
        "\n",
        "$$\n",
        "\\begin{cases}\n",
        "a_0 = a_1 &= 1 \\\\\n",
        "a_{n+1} &= a_n + a_{n-1}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "or in matrix form:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_n \\\\\n",
        "a_{n+1}\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0 & 1 \\\\\n",
        "1 & 1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "a_{n-1} \\\\\n",
        "a_{n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Is there a better way of generating the sequence? If we try to do the matrix multiplication repeatedly, we'll find that:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "a_n \\\\\n",
        "a_{n+1}\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0 & 1 \\\\\n",
        "1 & 1\n",
        "\\end{bmatrix}^n\n",
        "\\begin{bmatrix}\n",
        "1 \\\\\n",
        "1\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "which looks beautiful but then if you actually try to multiply this out, you realise you're still performing the same calculations. Are we out of luck?\n",
        "\n",
        "Of course not. Let:\n",
        "\n",
        "$$\n",
        "P =\n",
        "\\begin{bmatrix}\n",
        "2 & 2 \\\\\n",
        "1 + \\sqrt{5} & 1 - \\sqrt{5}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "which means\n",
        "\n",
        "$$\n",
        "P^{-1} = \\frac{1}{4\\sqrt{5}}\n",
        "\\begin{bmatrix}\n",
        "\\sqrt{5} - 1 & 2 \\\\\n",
        "\\sqrt{5} + 1 & -2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Now, if $A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$, then we can write:\n",
        "\n",
        "$$\n",
        "P^{-1} A P =\n",
        "\\begin{bmatrix}\n",
        "\\frac{1 + \\sqrt{5}}{2} & 0 \\\\\n",
        "0 & \\frac{1 - \\sqrt{5}}{2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "This is a _diagonal_ matrix! And we know from a previous session that powers of diagonal matrices just square the entries in the diagonal. Hence:\n",
        "\n",
        "$$\n",
        "(P^{-1} A P)^n = (P^{-1} A P)(P^{-1} A P)\\dots(P^{-1} A P)\n",
        "= (P^{-1} A^n P)\n",
        "$$\n",
        "\n",
        "since all the $P P^{-1} = P^{-1} P = I$.\n",
        "\n",
        "Solving for $A^n$ then by multiplying $P$'s and $P^{-1}$'s, we get:\n",
        "\n",
        "$$\n",
        "A^n = P \\, (P^{-1} A P)^n \\, P^{-1}\n",
        "$$\n",
        "\n",
        "And expanding this out by substituting the values (and letting $\\lambda_1 = \\tfrac{1+\\sqrt{5}}{2}$, $\\lambda_2 = \\tfrac{1-\\sqrt{5}}{2}$), we obtain:\n",
        "\n",
        "$$\n",
        "A^n = \\frac{1}{2\\sqrt{5}}\n",
        "\\begin{bmatrix}\n",
        "\\lambda_1^n (\\sqrt{5} - 1) + \\lambda_2^n (\\sqrt{5} + 1) &\n",
        "2\\lambda_1^n - 2\\lambda_2^n \\\\\n",
        "\\lambda_1^{n+1} (\\sqrt{5} - 1) + \\lambda_2^{n+1} (\\sqrt{5} + 1) &\n",
        "2\\lambda_1^{n+1} - 2\\lambda_2^{n+1} \\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "0 & 1 \\\\\n",
        "1 & 1\n",
        "\\end{bmatrix}^n\n",
        "$$\n",
        "\n",
        "And so by our original matrix equation (the one with $A^n$), we can obtain a _direct_ formula for the nth Fibonacci number $a_n$:\n",
        "\n",
        "$$\n",
        "a_n = \\left( \\frac{5 + \\sqrt{5}}{10} \\lambda_1^n \\right) + \\left( \\frac{5 - \\sqrt{5}}{10} \\lambda_2^n \\right)\n",
        "$$\n",
        "\n",
        "Isn't this remarkable? We only need to calculate $\\lambda_1$ and $\\lambda_2$ now to get to _any_ Fibonacci number, no need to bother unrolling the recurrence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGihQobw4hUZ"
      },
      "source": [
        "## Definition: (eigenvector, eigenvalue, multiplicity)\n",
        "\n",
        "Let $V$ be a vector space and $T: V \\to V$ be a linear transformation (here, we blur the notation between $T$ the transformation and its corresponding matrix $T$).\n",
        "\n",
        "Then a _nonzero_ vector $\\vec{v}$ such that:\n",
        "\n",
        "$$\n",
        "T{\\vec{v}} = \\lambda \\vec{v}\n",
        "$$\n",
        "\n",
        "for some scalar $\\lambda$ is called an **eigenvector** of $T$, and $\\lambda$ is its corresponding **eigenvalue**. The dimension of the set $\\{\\vec{v} | T{\\vec{v}} = \\lambda \\vec{v}\\}$, called the **eigenspace**, is called the **multiplicity** of $\\lambda$.\n",
        "\n",
        "---\n",
        "\n",
        "In other words, we are now talking about the matrix equation $A\\vec{x} = \\lambda \\vec{x}$.\n",
        "\n",
        "What this set up tells us is that we are interested in finding the vectors for which applying $T$ results in at most a scaling of said vectors. In some sense, these _eigenvectors_ represent those vectors that are merely stretched, never rotated or sheared. They are directions preserved by $T$, and so knowing what they are goes a long way towards characterising what $T$ really does.\n",
        "\n",
        "Also note that $\\lambda$ may be a complex number.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KszfO7P34hUa"
      },
      "source": [
        "![img](https://i.postimg.cc/dQxjTGjx/shear.png)\n",
        "\n",
        "_Fig. 1: In this image (from [TreyGreer62](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#/media/File:Mona_Lisa_eigenvector_grid.png) of Wikipedia), the red arrow is sheared by the transformation but the blue arrow stays in the same direction. Therefore, it is an eigenvector._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WO0V1SE4hUa"
      },
      "source": [
        "## Problem 4: Eigenproblems\n",
        "\n",
        "a. Explain why only square matrices can have eigenvectors.\n",
        "\n",
        "b. Show that if $T{\\vec{v}} = \\lambda \\vec{v}$, then $T^k{\\vec{v}} = \\lambda^k \\vec{v}$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer to Problem 4.a\n",
        "- In a computational perspective, you can't really compute eigenvectors and eigenvalues without the determinant, so only square matrices who have a determinant (unlike non square matrices) can have eigenvectors.\n",
        "- And in an intuition and geometric perspective, only square matrices represents transformations within the same dimensional space in which the concept of eigenvectors can exist, but in non square matrices they represent transformations that change the dimensionality of space, making it impossible for vectors to retain their direction after the transformation, thus invalidating the concept of eigenvectors.\n",
        "### Answer to Problem 4.b\n",
        "We can prove this is true using mathematical induction.\n",
        "\n",
        "For the base case:\n",
        "- $T{\\vec{v}} = \\lambda \\vec{v}$ (k=1):\n",
        "This is the given condition/assumption already, so the base case holds.\n",
        "\n",
        "Inductive Hypothesis:\n",
        "- Assume that $T^k\\vec{v} = \\lambda^k \\vec{v}$ for some positive integer $k$.\n",
        "\n",
        "Inductive Step:\n",
        "- For the next steps, we want to prove that $T^{k+1}\\vec{v} = \\lambda^{k+1} \\vec{v}$. We can do this by using existing linear transformation properties and inference.\n",
        "- Steps:\n",
        " - $T^{k+1}\\vec{v} = T(T^{k} \\vec{v})$\n",
        " - $T^{k+1}\\vec{v} = T(\\lambda^{k} \\vec{v})$\n",
        "   - as per our inductive hypothesis\n",
        " - $T^{k+1}\\vec{v} = \\lambda^{k}(T\\vec{v})$\n",
        "   - Since $T$ is a linear transformation, it follows the distributive property:\n",
        " - $T^{k+1}\\vec{v} = \\lambda^{k}(\\lambda\\vec{v})$\n",
        "   - As per the base case\n",
        " - $T^{k+1}\\vec{v} = \\lambda^{k+1} \\vec{v}$\n",
        "   - Simplifying the terms in the right side\n",
        "\n",
        "Therefore, we have shown that if the statement holds for k, it also holds for k + 1.\n",
        "\n",
        "As per principle of mathematical induction, the statement holds for all positive integers k. Or in other words, if $T{\\vec{v}} = \\lambda \\vec{v}$, then $T^k{\\vec{v}} = \\lambda^k \\vec{v}$."
      ],
      "metadata": {
        "id": "C-G4Q1MaMHwR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7LdDtaC4hUa"
      },
      "source": [
        "\n",
        "## Definition: (eigenbasis)\n",
        "\n",
        "A basis for a vector space $V$ is an **eigenbasis** of $V$ for a linear transformation $T$ if each element of the basis is an eigenvector of $T$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xjV2-QD4hUa"
      },
      "source": [
        "## Problem 5: Demystifying the Fibonacci formula\n",
        "\n",
        "a. Verify that the columns of $P$ in the example above form an eigenbasis of $\\mathbb{R}^2$ for the linear transformation represented by $A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$\n",
        "\n",
        "b. Verify that the diagonals $\\lambda_1$ and $\\lambda_2$ of $P^{-1} A P$ are the corresponding eigenvalues of the eigenvectors above.\n",
        "\n",
        "c. Plot the first eigenvector $\\begin{bmatrix} 2 \\\\ 1 + \\sqrt{5} \\end{bmatrix}$ of $P$ and the first 10 Fibonacci vectors $\\begin{bmatrix} a_n \\\\ a_{n+1} \\end{bmatrix}$. How are they related?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JU2gDV67215",
        "outputId": "c4766622-799b-47d0-d23b-98cd6956be4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[ 0.61803399, -1.61803399],\n",
              "        [ 1.        ,  1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-3acdc96b4cc1f3ae",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "ygvdYQZh4hUg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "# P_val = 1 + math.sqrt(5)\n",
        "sqr_5 = math.sqrt(5)\n",
        "\n",
        "P_2 = np.matrix([\n",
        "    [.5*(-1+sqr_5),.5*(-1-sqr_5)],\n",
        "     [1,1]]\n",
        ")\n",
        "P_2\n",
        "\n",
        "A = np.matrix([[0,1], [1,1]])\n",
        "P = np.matrix([[2, 2], [1+sqr_5,1 - sqr_5 ]])\n",
        "\n",
        "eigenvalues = np.matrix([(1 +sqr_5)/2 ,(1 -sqr_5)/2])\n",
        "\n",
        "P_2 = np.matrix([\n",
        "    [.5*(-1+sqr_5),.5*(-1-sqr_5)],\n",
        "     [1,1]]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution for Problem 5.a\n",
        "# To check if P is an eigenbasis, we check the following:\n",
        "# - first if its an eigenvector in the first place and then\n",
        "# - check if its linearly independent.\n",
        "\n",
        "# Check first if its an eigenvector\n",
        "# we can do this by checking if both columns equals to A(x) = z(x), where z is the given eigenvalues.\n",
        "a = A * P[:,0]\n",
        "b =  eigenvalues[0,0] * P[:,0]\n",
        "print(f'{a.T} == {b.T} ') # Note: I just transposed to make it slightly easier to read/visualize.\n",
        "\n",
        "a_a = A * P[:,1]\n",
        "b_b = eigenvalues[0,1] * P[:,1]\n",
        "print(f'{a_a.T} == {b_b.T} ') # Note: I just transposed to make it slightly easier to read/visualize.\n",
        "\n",
        "# Next, check if these two vectors are linearly independent, you can check these either by\n",
        "# - checking its determinant is non zero or creating an augmented matrix and solving the combination of both for the zero vector.\n",
        "# - Add the zero vector and compute towards the zero vector\n",
        "P_determinant = np.linalg.det(P)\n",
        "print(f'Solution #1: Determinant is {P_determinant}, since its not 0, both vectors are linearly independent\\n which proves it spans all of R^2, thus making it a valid basis vector.')\n",
        "print('Solution #2:As suggested, you can also find this by computing both vectors in an augmented matrix towards the zero vector')\n",
        "print('Demonstrated as follows:')\n",
        "P_sample = np.matrix([[2, 2,0], [1+sqr_5,1 - sqr_5,0 ]])\n",
        "\n",
        "operation_1 = np.matrix(\n",
        "    [[1/2, 0],\n",
        "    [0., 1]],\n",
        ")\n",
        "P_sample = operation_1 * P_sample\n",
        "print('Matrix #1 after Operation 1:')\n",
        "display(P_sample)\n",
        "\n",
        "operation_2 = np.matrix(\n",
        "    [[1, 0],\n",
        "    [ -P_sample[1,0], 1]], # get the value to minus from existing matrix, you apparently need to do this due to precision\n",
        ")\n",
        "P_sample = operation_2 * P_sample\n",
        "print('Matrix #1 after Operation 2:')\n",
        "display(P_sample)\n",
        "\n",
        "operation_3 = np.matrix(\n",
        "    [[1, 0],\n",
        "    [ 0, (1 / P_sample[1,1])]], # multiply by its reciprocal,\n",
        "    # get the value to minus from existing matrix, you apparently need to do this due to precision\n",
        ")\n",
        "P_sample = operation_3 * P_sample\n",
        "print('Matrix #1 after Operation 3:')\n",
        "display(P_sample)\n",
        "operation_4 = np.matrix(\n",
        "    [[1, -1],\n",
        "    [ 0, 1]], # multiply by its reciprocal,\n",
        "    # get the value to minus from existing matrix, you apparently need to do this due to precision\n",
        ")\n",
        "P_sample = operation_4 * P_sample\n",
        "print('Matrix #1 after Operation 4:')\n",
        "display(P_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "PPsF9hV4UQLu",
        "outputId": "59be9483-d751-46ee-f707-13871f156807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3.23606798 5.23606798]] == [[3.23606798 5.23606798]] \n",
            "[[-1.23606798  0.76393202]] == [[-1.23606798  0.76393202]] \n",
            "Solution #1: Determinant is -8.944271909999157, since its not 0, both vectors are linearly independent\n",
            " which proves it spans all of R^2, thus making it a valid basis vector.\n",
            "Solution #2:As suggested, you can also find this by computing both vectors in an augmented matrix towards the zero vector\n",
            "Demonstrated as follows:\n",
            "Matrix #1 after Operation 1:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1.        ,  1.        ,  0.        ],\n",
              "        [ 3.23606798, -1.23606798,  0.        ]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #1 after Operation 2:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[ 1.        ,  1.        ,  0.        ],\n",
              "        [ 0.        , -4.47213595,  0.        ]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #1 after Operation 3:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[1., 1., 0.],\n",
              "        [0., 1., 0.]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix #1 after Operation 4:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "matrix([[1., 0., 0.],\n",
              "        [0., 1., 0.]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer to Problem 5.b\n",
        "Although we can just plug the existing values of $\\lambda_1$ and $\\lambda_2$ and use the following equation, $A\\vec{x} = \\lambda \\vec{x}$ to prove this is true,\n",
        "\n",
        "a more thorough and believable way to verify that $\\lambda_1$ and $\\lambda_2$ are the eigenvectors for the ones above, we can actually just derive it as we would normally would!\n",
        "\n",
        "So to get the eigenvalues of a transformation, we use the following formula:\n",
        "$det(A - (\\lambda)I)=0$\n",
        "\n",
        "Using this formula we get:\n",
        "- $det(A - (\\lambda)I) = 0$\n",
        "- $det(\\begin{bmatrix} 0 - λ & 1 \\\\ 1 & 1-λ\\end{bmatrix}) = 0$\n",
        "- $ λ^2 - λ -1  = 0$\n",
        "- Deriving the solutions for this quadratic results in the following, which is the same as the one given above!\n",
        " - $λ^1 = \\frac{1+\\sqrt{5}}{2}$\n",
        " - $λ^2 = \\frac{1-\\sqrt{5}}{2}$\n",
        "\n"
      ],
      "metadata": {
        "id": "7XSd5Xxn71Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = P[:,0] * eigenvalues[0,0]"
      ],
      "metadata": {
        "id": "XCWnyH45oDKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = a[:,0] * eigenvalues[0,0]\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uwinVSsoMB_",
        "outputId": "215d0826-cfd9-4f71-8812-29e04439d8e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[22.18033989],\n",
              "        [35.88854382]])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer to Problem 5.c\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "test_x = []\n",
        "test_y = []\n",
        "for i in range(2,20):\n",
        "  val = P[:,0] * i\n",
        "  test_x.append(val[0,0])\n",
        "  test_y.append(val[1,0])\n",
        "\n",
        "# new_val = P[:,0] * eigenvalues[0,0]\n",
        "# test_x.append(new_val[0,0])\n",
        "# test_y.append(new_val[1,0])\n",
        "# for i in range(3,9):\n",
        "#   new_val = new_val * eigenvalues[0,0]\n",
        "#   test_x.append(new_val[0,0])\n",
        "#   test_y.append(new_val[1,0])\n",
        "\n",
        "\n",
        "x_P = [P[0,0]]\n",
        "y_P = [P[1,0]]\n",
        "x_fibonacci_vectors = [1,1,2,3,5,8,13,21,34,55]\n",
        "y_fibonacci_vectors = [1,2,3,5,8,13,21,34,55,89]\n",
        "layout = go.Layout(\n",
        "    title = \"Demistified Eigenvector\",\n",
        "    yaxis = {\"title\": \"y\"},\n",
        "    xaxis = {\"title\": \"x\"},\n",
        "    margin = dict(l=40,r=40,t=60,b=40)\n",
        ")\n",
        "fig = go.Figure(layout = layout)\n",
        "eigenvector_trace = go.Scatter(\n",
        "            x = x_P,\n",
        "            y = y_P,\n",
        "            mode = \"markers\",\n",
        "            name = \"Eigenvector\",\n",
        "        )\n",
        "fig.add_traces(eigenvector_trace)\n",
        "scaled_eigenvector_trace = go.Scatter(\n",
        "            x = test_x,\n",
        "            y = test_y,\n",
        "            mode = \"markers\",\n",
        "            name = \"Scaled Eigenvector\",\n",
        "        )\n",
        "fig.add_traces(scaled_eigenvector_trace)\n",
        "fibonnaci_trace = go.Scatter(\n",
        "            x = x_fibonacci_vectors,\n",
        "            y = y_fibonacci_vectors,\n",
        "            mode = \"markers\",\n",
        "            name = f\"Fibonacci Numbers\",\n",
        "        )\n",
        "fig.add_traces(fibonnaci_trace)\n",
        "print(\"\"\"Plotting both vectors let us to see that these two vectors are closely related,\n",
        "or in other words, their range of possible higher values are very close and they point at exactly the same direction.\n",
        "To better visualize this, I made two graphs using the possible values of both vectors for higher values.\n",
        "In the first graph, we can see very clearly that some of the eigenvector's values  are very close,\n",
        "hinting that we can derive possible fibonnaci numbers in the same line.\n",
        "And finally in the second graph,\n",
        "it just shows very clearly that they are almost in the same line with only very little difference in direction.\n",
        "In conclusion, plotting the eigenvector against the expected fibonacci numbers, shows us clearly that because they are similar vectors,\n",
        "we can derive possible values interchangeably from one vector to another.\n",
        "\"\"\")\n",
        "fig.show()\n",
        "fig = go.Figure(layout = layout)\n",
        "eigenvector_trace = go.Scatter(\n",
        "            x = x_P,\n",
        "            y = y_P,\n",
        "            mode = \"markers\",\n",
        "            name = \"Eigenvector\",\n",
        "        )\n",
        "fig.add_traces(eigenvector_trace)\n",
        "scaled_eigenvector_trace = go.Scatter(\n",
        "            x = test_x,\n",
        "            y = test_y,\n",
        "            mode = \"markers\",\n",
        "            name = \"Scaled Eigenvector\",\n",
        "        )\n",
        "fig.add_traces(scaled_eigenvector_trace)\n",
        "fibonnaci_trace = go.Scatter(\n",
        "            x = x_fibonacci_vectors,\n",
        "            y = y_fibonacci_vectors,\n",
        "            mode = \"lines\",\n",
        "            name = f\"Fibonacci Numbers\",\n",
        "        )\n",
        "fig.add_traces(fibonnaci_trace)\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HLW3WpUv75f0",
        "outputId": "930d15ba-fe8b-42dc-8bbb-8fcb50f9068f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Plotting both vectors let us to see that these two vectors are closely related,\n",
            "or in other words, their range of possible higher values are very close and they point at exactly the same direction.\n",
            "To better visualize this, I made two graphs using the possible values of both vectors for higher values.\n",
            "In the first graph, we can see very clearly that some of the eigenvector's values  are very close,\n",
            "hinting that we can derive possible fibonnaci numbers in the same line.\n",
            "And finally in the second graph, \n",
            "it just shows very clearly that they are almost in the same line with only very little difference in direction.\n",
            "In conclusion, plotting the eigenvector against the expected fibonacci numbers, shows us clearly that because they are similar vectors,\n",
            "we can derive possible values interchangeably from one vector to another.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"2a53e61e-dd38-4cdf-a37a-15fc4a2eb670\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2a53e61e-dd38-4cdf-a37a-15fc4a2eb670\")) {                    Plotly.newPlot(                        \"2a53e61e-dd38-4cdf-a37a-15fc4a2eb670\",                        [{\"mode\":\"markers\",\"name\":\"Eigenvector\",\"x\":[2.0],\"y\":[3.23606797749979],\"type\":\"scatter\"},{\"mode\":\"markers\",\"name\":\"Scaled Eigenvector\",\"x\":[4.0,6.0,8.0,10.0,12.0,14.0,16.0,18.0,20.0,22.0,24.0,26.0,28.0,30.0,32.0,34.0,36.0,38.0],\"y\":[6.47213595499958,9.70820393249937,12.94427190999916,16.18033988749895,19.41640786499874,22.65247584249853,25.88854381999832,29.12461179749811,32.3606797749979,35.59674775249769,38.83281572999748,42.068883707497264,45.30495168499706,48.54101966249685,51.77708763999664,55.01315561749642,58.24922359499622,61.48529157249601],\"type\":\"scatter\"},{\"mode\":\"markers\",\"name\":\"Fibonacci Numbers\",\"x\":[1,1,2,3,5,8,13,21,34,55],\"y\":[1,2,3,5,8,13,21,34,55,89],\"type\":\"scatter\"}],                        {\"margin\":{\"b\":40,\"l\":40,\"r\":40,\"t\":60},\"title\":{\"text\":\"Demistified Eigenvector\"},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2a53e61e-dd38-4cdf-a37a-15fc4a2eb670');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"5f6d6005-3b61-4421-9042-4ce539ef3459\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"5f6d6005-3b61-4421-9042-4ce539ef3459\")) {                    Plotly.newPlot(                        \"5f6d6005-3b61-4421-9042-4ce539ef3459\",                        [{\"mode\":\"markers\",\"name\":\"Eigenvector\",\"x\":[2.0],\"y\":[3.23606797749979],\"type\":\"scatter\"},{\"mode\":\"markers\",\"name\":\"Scaled Eigenvector\",\"x\":[4.0,6.0,8.0,10.0,12.0,14.0,16.0,18.0,20.0,22.0,24.0,26.0,28.0,30.0,32.0,34.0,36.0,38.0],\"y\":[6.47213595499958,9.70820393249937,12.94427190999916,16.18033988749895,19.41640786499874,22.65247584249853,25.88854381999832,29.12461179749811,32.3606797749979,35.59674775249769,38.83281572999748,42.068883707497264,45.30495168499706,48.54101966249685,51.77708763999664,55.01315561749642,58.24922359499622,61.48529157249601],\"type\":\"scatter\"},{\"mode\":\"lines\",\"name\":\"Fibonacci Numbers\",\"x\":[1,1,2,3,5,8,13,21,34,55],\"y\":[1,2,3,5,8,13,21,34,55,89],\"type\":\"scatter\"}],                        {\"margin\":{\"b\":40,\"l\":40,\"r\":40,\"t\":60},\"title\":{\"text\":\"Demistified Eigenvector\"},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('5f6d6005-3b61-4421-9042-4ce539ef3459');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wknJR3Lx4hUg"
      },
      "source": [
        "## Definition: (diagonalizable matrix)\n",
        "\n",
        "A matrix $A$ is **diagonalisable** if there exists a matrix $P$ such that $P^{-1} A P$ is diagonal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFxOCvWN4hUg"
      },
      "source": [
        "## Proposition IV: Diagonalization\n",
        "\n",
        "Let $A$ be an $n \\times n$ matrix and $P = [\\vec{v_1}, \\dots, \\vec{v_n}]$ an invertible $n \\times n$ matrix.\n",
        "\n",
        "Then the following are true:\n",
        "\n",
        "1. The eigenvalues of $A$ and the eigenvalues of $P^{-1} A P$ coincide.\n",
        "2. If $P^{-1} A P$ is a diagonal matrix with diagonal entries $\\lambda_i$, the columns of P are eigenvectors of $A$, with eigenvalues $\\lambda_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiuDX6UG4hUg"
      },
      "source": [
        "## Problem (bonus): Prove Proposition IV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "cell-659d4fb3a9c9596e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "23pfXgoA4hUg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E9Dq9z84hUh"
      },
      "source": [
        "## Proposition V: Eigenvectors with distinct eigenvalues are linearly\n",
        "\n",
        "---\n",
        "\n",
        "independent\n",
        "\n",
        "If $A: V \\to V$ is a linear transformation and $\\{\\vec{v_i}\\}$ are eigenvectors of $A$ with distinct eigenvalues $\\{\\lambda_i\\}$, then $\\{\\vec{v_i}\\}$ are linearly independent.\n",
        "\n",
        "---\n",
        "\n",
        "In particular, there are at most $\\text{dim }V = n$ eigenvalues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wShbJ6D4hUh"
      },
      "source": [
        "## Problem 6: Prove Proposition V by contradiction.\n",
        "\n",
        "Hint: you will need to use $\\lambda_j I - A$, where $0 \\leq j \\leq i$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### My Proof:\n",
        "the way to prove Proposition V is true by contradiction is going with the assumption that the opposite is true, which is all eigenvectors are linearly dependent and find a contradiction. I do this through the steps below:\n",
        "- Suppose that $\\{\\vec{v_i}\\}$ were linearly dependent.\n",
        "- if it is there should be a eigenvector in  $\\{\\vec{v_i}\\}$ where it results in the linear combination of all the vectors preceding it:\n",
        " - $\\vec{v}_{j+1} = c_1v_1 + c_2v_2 ... + c_jv_j $ (equation #1)\n",
        "- Since $\\vec{v}_{j+1}$ is an eigenvector and all of what is being summed to it is an eigenvector, all vectors in $\\{\\vec{v_i}\\}$ can be both represented as $\\lambda v$, therefore:\n",
        " - $\\lambda_{j+1}\\vec{v}_{j+1} = (\\lambda_{1})c_1v_1 + (\\lambda_{2})c_2v_2 ... + (\\lambda_{j})c_jv_j $ (equation #2)\n",
        "- now, if we were to multiply equation #1 with $\\lambda_{j+1}$ we eventually get:\n",
        " - $(\\lambda_{j+1})\\vec{v}_{j+1} = (\\lambda_{j+1})c_1v_1 + (\\lambda_{j+1})c_2v_2 ... + (\\lambda_{j+1})c_jv_j $ (equation #1)\n",
        "- we can then get the difference of equation #2 and #1 since they are equa and group like terms to get the following:\n",
        " -  $0 =  c_1(\\lambda_1 -\\lambda_{j+1})v_1 + c_2(\\lambda_2 -\\lambda_{j+1})v_2 + ... + c_j(\\lambda_j -\\lambda_{j+1})v_j$\n",
        "\n",
        "Reading upon the new equation derived from the difference of two equations, we  infer a few things:\n",
        "-  $(\\lambda_j -\\lambda_{j+1})$ - each combination pair in this equation will only result in a non-zero value since both $\\lambda_j$ and $\\lambda_{j+1}$ are distinct.\n",
        "- all vectors of $v_j$ in this equation can only be non-zero due to the definition of eigenvectors.\n",
        "\n",
        "Infering from these two information derived, then in this equation  $c_j$ must be zero for this equation to work.\n",
        "\n",
        "And thus, **this implies that the assumption $\\vec{v}_{j+1}$ can be a linear combination of some vectors, or in other words $\\vec{v}_{j+1} c_1v_1 + c_2v_2 ... + c_jv_j $ is not true at all. Therefore, all vectors in  $\\{\\vec{v_i}\\}$ must have been linearly independent after all.**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X184NOnBe4ou"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYDzNsAz4hUh"
      },
      "source": [
        "---\n",
        "\n",
        "So what have we done? We've seen that to study a linear transformation, it is enough to look at its eigenbasis, and furthermore, that there is a process called _diagonalisation_ that simplifies certain calculations involving a transformation without changing what it is doing.\n",
        "\n",
        "This was particularly salient in the Fibonacci problem when we constructed $P$ from the eigenvectors of $A = \\begin{bmatrix} 0 & 1 \\\\ 1 & 1 \\end{bmatrix}$ and arrived at a closed-form formula for $a_n$ after forming $P^{-1} A P$.\n",
        "\n",
        "To understand what an eigenbasis meant, it was necessary for us to understand how and when we can combine vectors and what kind of mathematical objects we can build from them.\n",
        "\n",
        "But how do we find the eigenbasis for a linear transformation in the first place? And how does that help us understand neural networks? Stay tuned for more. 🙂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkRPE2ZZ4hUh"
      },
      "source": [
        "## Additional resources\n",
        "\n",
        "* _The Essence of Linear Algebra_, by 3Blue1Brown: [LINK](https://www.3blue1brown.com/topics/linear-algebra) (2-3 hours)\n",
        "* _Orthogonal Vectors and Subspaces_, an MIT OpenCourseWare lecture video by G. Strang: [LINK](https://openlearninglibrary.mit.edu/courses/course-v2:OCW+18.06SC+2T2019/courseware/201102cc58fd4300b62bdba016238741/587d3844f821441d94418db8b125357e/?activate_block_id=block-v1%3AOCW%2B18.06SC%2B2T2019%2Btype%40sequential%2Bblock%40587d3844f821441d94418db8b125357e) (~50 mins)\n",
        "* _Eigenvalues and Eigenvectors_, by G. Strang: [LINK](https://openlearninglibrary.mit.edu/courses/course-v1:OCW+18.06SC+2T2019/courseware/201102cc58fd4300b62bdba016238741/ff5e10d34f074173a7ace8d54d0f5238/?activate_block_id=block-v1%3AOCW%2B18.06SC%2B2T2019%2Btype%40sequential%2Bblock%40ff5e10d34f074173a7ace8d54d0f5238) (~50 mins)\n",
        "* _Diagonalization and Powers of A_, by G. Strang: [LINK](https://openlearninglibrary.mit.edu/courses/course-v1:OCW+18.06SC+2T2019/courseware/201102cc58fd4300b62bdba016238741/a5d0b53714b646839c528115ef3157b7/?activate_block_id=block-v1%3AOCW%2B18.06SC%2B2T2019%2Btype%40sequential%2Bblock%40a5d0b53714b646839c528115ef3157b7) (~50 mins)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}